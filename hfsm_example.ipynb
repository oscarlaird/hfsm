{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "introductions = [\n",
    "    (\"Hi, I'm Michael Johnson, a graduate student at Stanford University.\", \"Michael Johnson\", \"Stanford University\"),\n",
    "    (\"Greetings, I'm Emma Wilson, and I'm studying at Harvard University.\", \"Emma Wilson\", \"Harvard University\"),\n",
    "    (\"My name is Sarah Lee, and I'm currently pursuing my PhD at MIT.\", \"Sarah Lee\", \"MIT\"),\n",
    "    (\"I am Daniel Martinez, a senior researcher at the University of Cambridge.\", \"Daniel Martinez\", \"University of Cambridge\"), \n",
    "    (\"Hello everyone, I'm Olivia Brown, a faculty member at Yale University.\", \"Olivia Brown\", \"Yale University\"),\n",
    "    (\"I'm Ethan Taylor, an undergraduate student at the University of Oxford.\", \"Ethan Taylor\", \"University of Oxford\"), \n",
    "    (\"My name is Mia Anderson, and I am a postdoctoral fellow at Caltech.\", \"Mia Anderson\", \"Caltech\"), \n",
    "    (\"Hello, I'm Sophia Hernandez, an assistant professor at Princeton University.\", \"Sophia Hernandez\", \"Princeton University\"),\n",
    "    (\"I am Lucas White, and I specialize in astrophysics at the University of Edinburgh.\", \"Lucas White\", \"University of Edinburgh\"),\n",
    "    (\"Greetings, my name is Charlotte Johnson, and I'm a lecturer at the University of Tokyo.\", \"Charlotte Johnson\", \"University of Tokyo\"),\n",
    "    (\"I'm Amelia Martinez, a master's student in computer science at the University of Toronto.\", \"Amelia Martinez\", \"University of Toronto\"),\n",
    "    (\"Hello, I'm William Davis, currently a research intern at Columbia University.\", \"William Davis\", \"Columbia University\"),\n",
    "    (\"My name is Isabella Garcia, and I am studying environmental science at the University of Chicago.\", \"Isabella Garcia\", \"University of Chicago\"),\n",
    "    (\"I'm Noah Smith, and I'm conducting research in molecular biology at the University of California, Berkeley.\", \"Noah Smith\", \"University of California, Berkeley\"),\n",
    "    (\"Hi, I am Ava Wilson, an exchange student at the University of Hong Kong.\", \"Ava Wilson\", \"University of Hong Kong\"),\n",
    "    (\"I'm Sophia Lee, and I'm completing my residency in medicine at Johns Hopkins University.\", \"Sophia Lee\", \"Johns Hopkins University\"),\n",
    "    (\"Hello, I'm Benjamin Clark, a graduate in mechanical engineering from the University of Michigan.\", \"Benjamin Clark\", \"University of Michigan\"),\n",
    "    (\"I am Isabella Johnson, a junior researcher at the Imperial College London.\", \"Isabella Johnson\", \"Imperial College London\"),\n",
    "    (\"My name is James Anderson, and I'm specializing in quantum computing at the University of Paris.\", \"James Anderson\", \"University of Paris\"),\n",
    "    (\"Hello, I'm Emily Thompson, and I'm studying art history at the University of Rome.\" , \"Emily Thompson\", \"University of Rome\"),\n",
    "]\n",
    "# uppercase the student names\n",
    "introductions = [(desc, name.upper(), uni) for desc, name, uni in introductions]\n",
    "# prompt\n",
    "prompt = \"What is the name of the student mentioned in the text?\\n\\nText: \"\n",
    "# add ids\n",
    "introductions = [(str(i), prompt + desc, name) for i, (desc, name, _) in enumerate(introductions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb0f1f072c84f6bbd96b4755a5ccac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a863553d93004b99aacee5958d92455c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a095329127ec4ff3b7d0cede8c09cf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a5bbf49d584b209c693ae7d6f0f8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc45f190890f44c2af49c84410f2c298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3afd3ebb88e4ba6a70ef77b6215e513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca0cc4a5505456aa40c7df064c5fbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/544 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a hugginface dataset from the introductions\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(introductions, columns=[\"id\",\"dialogue\", \"summary\"])\n",
    "# to hf\n",
    "import datasets\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset\n",
    "# 10/5/5 split: train, validation, test\n",
    "train_valid_test_split = dataset.train_test_split(test_size=5)\n",
    "train_valid_split = train_valid_test_split[\"train\"].train_test_split(test_size=5)\n",
    "final_splits = datasets.DatasetDict({\n",
    "    \"train\": train_valid_split[\"train\"],\n",
    "    \"validation\": train_valid_split[\"test\"],\n",
    "    \"test\": train_valid_test_split[\"test\"]\n",
    "})\n",
    "# create a validation split\n",
    "\n",
    "import huggingface_hub\n",
    "final_splits.push_to_hub(\"introductions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Johnson\n"
     ]
    }
   ],
   "source": [
    "# get Flan-T5 running locally\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Your prompt\n",
    "prompt = \"What is the name of the student in the text? Answer in all CAPS\\nText: Hi, I'm Michael Johnson, a graduate student at Stanford University\\nAnswer: \"\n",
    "\n",
    "# Encode the prompt and generate a response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/oscar/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::457891790242:role/SageMaker-power1\n",
      "sagemaker bucket: sagemaker-us-east-1-457891790242\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "sagemaker_session_bucket\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='SageMaker-power1')['Role']['Arn']\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/oscar/.config/sagemaker/config.yaml\n",
      "s3://jumpstart-cache-prod-us-east-1/huggingface-infer/prepack/v1.0.0/infer-prepack-huggingface-text2text-flan-t5-large.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_id, model_version, = (\n",
    "\"huggingface-text2text-flan-t5-large\",\n",
    "\"1.0.0\",\n",
    ")\n",
    "inference_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "region=None,\n",
    "framework=None, # automatically inferred from model_id\n",
    "image_scope=\"inference\",\n",
    "model_id=model_id,\n",
    "model_version=model_version,\n",
    "instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "#Create the SageMaker model instance\n",
    "model = Model(\n",
    "image_uri=deploy_image_uri,\n",
    "model_data=model_uri,\n",
    "role=role,\n",
    "predictor_cls=Predictor,\n",
    ")\n",
    "hyper_params = {\"batch_size\":4, \"max_length\":50, \"top_k\": 50, \"top_p\": 0.95, \"do_sample\": True}\n",
    "hyper_params_dict = {\"HYPER_PARAMS\":str(hyper_params)}\n",
    "\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4',\n",
       " 'dialogue': \"Hello everyone, I'm Olivia Brown, a faculty member at Yale University.\",\n",
       " 'summary': 'OLIVIA BROWN'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"oscarlaird/introductions\", split=\"test\")\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/oscar/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# sagemaker uses jsonl inputs\n",
    "# convert the dataset to jsonl\n",
    "import json\n",
    "test_data_file_name = \"./inputs.jsonl\"\n",
    "prompt = \"What is the name of the student in the text? Answer in all CAPS\\nText: \"\n",
    "suffix = \"\\nAnswer: \"\n",
    "with open(test_data_file_name, \"w+\") as outfile:\n",
    "    for record in test_dataset:\n",
    "        new_entry = {}\n",
    "        new_entry['id'] = record['id']\n",
    "        new_entry['text_inputs'] = prompt + record['dialogue'] + suffix\n",
    "        json.dump(new_entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# upload test_data_file to S3\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "input_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(test_data_file_name, input_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-11-26-00-23-42-619\n"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.p3.2xlarge\"\n",
    "batch_transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    output_path=output_s3_path,\n",
    "    accept=\"text/csv\", # \"application/jsonlines\", #?\n",
    "    strategy=\"SingleRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    max_payload=1,\n",
    "    env=hyper_params_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-jumpstart-2023-11-26-00-23-46-166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................\n",
      "Processing /opt/ml/model/code/lib/tokenizers/tokenizers-0.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Processing /opt/ml/model/code/lib/tokenizers/tokenizers-0.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Processing /opt/ml/model/code/lib/transformers/transformers-4.24.0-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.23.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.26.11)\n",
      "Processing /opt/ml/model/code/lib/transformers/transformers-4.24.0-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.23.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.24.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.26.11)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.0\n",
      "    Uninstalling tokenizers-0.13.0:\n",
      "      Successfully uninstalled tokenizers-0.13.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.17.0\n",
      "    Uninstalling transformers-4.17.0:\n",
      "      Successfully uninstalled transformers-4.17.0\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.0\n",
      "    Uninstalling tokenizers-0.13.0:\n",
      "      Successfully uninstalled tokenizers-0.13.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.17.0\n",
      "    Uninstalling transformers-4.17.0:\n",
      "      Successfully uninstalled transformers-4.17.0\n",
      "Successfully installed tokenizers-0.13.1 transformers-4.24.0\n",
      "Successfully installed tokenizers-0.13.1 transformers-4.24.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Warning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "Warning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "2023-11-26T00:32:39,791 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "MMS Home: /opt/conda/lib/python3.8/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /home/model-server/tmp\n",
      "Number of GPUs: 1\n",
      "Number of CPUs: 8\n",
      "Max heap size: 13643 M\n",
      "Python executable: /opt/conda/bin/python3.8\n",
      "Config file: /etc/sagemaker-mms.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Model Store: /.sagemaker/mms/models\n",
      "Initial Models: ALL\n",
      "Log dir: null\n",
      "Metrics dir: null\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "2023-11-26T00:32:39,791 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "MMS Home: /opt/conda/lib/python3.8/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /home/model-server/tmp\n",
      "Number of GPUs: 1\n",
      "Number of CPUs: 8\n",
      "Max heap size: 13643 M\n",
      "Python executable: /opt/conda/bin/python3.8\n",
      "Config file: /etc/sagemaker-mms.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Model Store: /.sagemaker/mms/models\n",
      "Initial Models: ALL\n",
      "Log dir: null\n",
      "Metrics dir: null\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 1\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Preload model: false\n",
      "Prefer direct buffer: false\n",
      "2023-11-26T00:32:39,858 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "2023-11-26T00:32:39,932 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\n",
      "2023-11-26T00:32:39,933 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "2023-11-26T00:32:39,934 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\n",
      "2023-11-26T00:32:39,934 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "2023-11-26T00:32:39,934 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.8.10\n",
      "2023-11-26T00:32:39,935 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "2023-11-26T00:32:39,940 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2023-11-26T00:32:39,950 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "2023-11-26T00:32:40,012 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "Model server started.\n",
      "2023-11-26T00:32:40,016 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "2023-11-26T00:32:40,016 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "2023-11-26T00:32:40,102 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:33512 \"GET /ping HTTP/1.1\" 200 9\n",
      "2023-11-26T00:32:40,111 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:33516 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "Default workers per model: 1\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Preload model: false\n",
      "Prefer direct buffer: false\n",
      "2023-11-26T00:32:39,858 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "2023-11-26T00:32:39,932 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\n",
      "2023-11-26T00:32:39,933 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "2023-11-26T00:32:39,934 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\n",
      "2023-11-26T00:32:39,934 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "2023-11-26T00:32:39,934 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.8.10\n",
      "2023-11-26T00:32:39,935 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "2023-11-26T00:32:39,940 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2023-11-26T00:32:39,950 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "2023-11-26T00:32:40,012 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "Model server started.\n",
      "2023-11-26T00:32:40,016 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "2023-11-26T00:32:40,016 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "2023-11-26T00:32:40,102 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:33512 \"GET /ping HTTP/1.1\" 200 9\n",
      "2023-11-26T00:32:40,111 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:33516 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "2023-11-26T00:32:40.118:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD\n",
      "2023-11-26T00:32:55,127 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000023-00000000-fd81a6b6870aec16-fd8ba4c5\n",
      "2023-11-26T00:32:55,129 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 15048\n",
      "2023-11-26T00:32:55,130 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "2023-11-26T00:32:55,133 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error\n",
      "2023-11-26T00:32:55,133 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1\n",
      "2023-11-26T00:32:55,133 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2023-11-26T00:32:55,134 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 234, in handle\n",
      "2023-11-26T00:32:55,134 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(self.model, input_data, content_type, accept)\n",
      "2023-11-26T00:32:55,134 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:33524 \"POST /invocations HTTP/1.1\" 400 14994\n",
      "2023-11-26T00:32:55,134 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/.sagemaker/mms/models/model/code/inference.py\", line 149, in transform_fn\n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(content_type or \"unknown\"))\n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ValueError: {\"error\": \"unsupported content type application/jsonlines\"}\n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/mms/service.py\", line 108, in predict\n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch, self.context)\n",
      "2023-11-26T00:32:55,127 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000023-00000000-fd81a6b6870aec16-fd8ba4c5\n",
      "2023-11-26T00:32:55,129 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 15048\n",
      "2023-11-26T00:32:55,130 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "2023-11-26T00:32:55,133 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error\n",
      "2023-11-26T00:32:55,133 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1\n",
      "2023-11-26T00:32:55,133 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2023-11-26T00:32:55,134 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 234, in handle\n",
      "2023-11-26T00:32:55,134 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(self.model, input_data, content_type, accept)\n",
      "2023-11-26T00:32:55,134 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:33524 \"POST /invocations HTTP/1.1\" 400 14994\n",
      "2023-11-26T00:32:55,134 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/.sagemaker/mms/models/model/code/inference.py\", line 149, in transform_fn\n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(content_type or \"unknown\"))\n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ValueError: {\"error\": \"unsupported content type application/jsonlines\"}\n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2023-11-26T00:32:55,135 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/mms/service.py\", line 108, in predict\n",
      "2023-11-26T00:32:55,136 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch, self.context)\n",
      "2023-11-26T00:32:55,137 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 243, in handle\n",
      "2023-11-26T00:32:55,137 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)\n",
      "2023-11-26T00:32:55,137 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: {\"error\": \"unsupported content type application/jsonlines\"} : 400\n",
      "2023-11-26T00:32:55,137 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 243, in handle\n",
      "2023-11-26T00:32:55,137 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)\n",
      "2023-11-26T00:32:55,137 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: {\"error\": \"unsupported content type application/jsonlines\"} : 400\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl: ClientError: 400\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl: \n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl: Message:\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl: {\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl:   \"code\": 400,\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl:   \"type\": \"InternalServerException\",\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl:   \"message\": \"{\\\"error\\\": \\\"unsupported content type application/jsonlines\\\"}\"\n",
      "2023-11-26T00:32:55.143:[sagemaker logs]: sagemaker-us-east-1-457891790242/batch_transform/input/inputs.jsonl: }\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job sagemaker-jumpstart-2023-11-26-00-23-46-166: Failed. Reason: ClientError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/oscar/dbi/hfsm/hfsm_example.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_transformer\u001b[39m.\u001b[39;49mtransform(input_s3_path, content_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mapplication/jsonlines\u001b[39;49m\u001b[39m\"\u001b[39;49m, split_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLine\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch_transformer\u001b[39m.\u001b[39mwait()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 0m create job\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 1m download model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 2m download data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 3m start processing\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 4m finish processing\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hfsm-0EdPmVNd-py3.10/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hfsm-0EdPmVNd-py3.10/lib/python3.10/site-packages/sagemaker/transformer.py:316\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, batch_data_capture_config, wait, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_transform_job \u001b[39m=\u001b[39m _TransformJob\u001b[39m.\u001b[39mstart_new(\n\u001b[1;32m    301\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    302\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     batch_data_capture_config,\n\u001b[1;32m    313\u001b[0m )\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_transform_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hfsm-0EdPmVNd-py3.10/lib/python3.10/site-packages/sagemaker/transformer.py:683\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, logs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m logs:\n\u001b[0;32m--> 683\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_transform_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    684\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_transform_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hfsm-0EdPmVNd-py3.10/lib/python3.10/site-packages/sagemaker/session.py:5130\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   5127\u001b[0m             state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   5129\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 5130\u001b[0m     _check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTransformJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   5131\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   5132\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hfsm-0EdPmVNd-py3.10/lib/python3.10/site-packages/sagemaker/session.py:6950\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   6945\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   6946\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6947\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6948\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6949\u001b[0m     )\n\u001b[0;32m-> 6950\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6951\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6952\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6953\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6954\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job sagemaker-jumpstart-2023-11-26-00-23-46-166: Failed. Reason: ClientError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "batch_transformer.transform(input_s3_path, content_type=\"application/jsonlines\", split_type=\"Line\")\n",
    "batch_transformer.wait()\n",
    "# 0m create job\n",
    "# 10m download model\n",
    "# 2m download data\n",
    "# 3m start processing\n",
    "# 4m finish processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/oscar/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_s3_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/oscar/dbi/hfsm/hfsm_example.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ms3\u001b[39;00m \u001b[39mimport\u001b[39;00m S3Downloader\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_file_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moutputs.jsonl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/oscar/dbi/hfsm/hfsm_example.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m S3Downloader\u001b[39m.\u001b[39mdownload(output_s3_path, \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_s3_path' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "output_file_name = \"outputs.jsonl\"\n",
    "S3Downloader.download(output_s3_path, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the output file\n",
    "batch_transform_result = []\n",
    "with open(output_file_name, \"r\") as outfile:\n",
    "    for line in outfile:\n",
    "        batch_transform_result.append(json.loads(line))\n",
    "print(batch_transform_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfsm-0EdPmVNd-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
